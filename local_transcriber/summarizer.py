"""
Local summarization module using Ollama (local LLM).
Generates summaries from transcription text without external API dependencies.
"""

import sys
import subprocess
import json
from pathlib import Path

# Add parent src directory to path for shared utilities
parent_dir = Path(__file__).parent.parent
sys.path.insert(0, str(parent_dir / "src"))


def check_ollama_available():
    """Check if Ollama is installed and running."""
    try:
        result = subprocess.run(
            ["ollama", "list"],
            capture_output=True,
            text=True,
            timeout=10
        )
        return result.returncode == 0
    except Exception:
        return False


def ollama_generate(prompt, model="llama3.2", timeout=120):
    """
    Generate text using Ollama CLI.
    
    Args:
        prompt: The prompt to send to the model
        model: Model name (default: llama3.2)
        timeout: Timeout in seconds
    
    Returns:
        str: Generated text or None on error
    """
    try:
        result = subprocess.run(
            ["ollama", "run", model, prompt],
            capture_output=True,
            text=True,
            timeout=timeout
        )
        
        if result.returncode == 0:
            return result.stdout.strip()
        else:
            print(f"âš ï¸ Ollama error: {result.stderr}")
            return None
    except subprocess.TimeoutExpired:
        print(f"âš ï¸ Ollama timeout after {timeout}s")
        return None
    except Exception as e:
        print(f"âš ï¸ Ollama error: {str(e)}")
        return None


class OllamaSummarizer:
    """
    Generate summaries using local Ollama LLM.
    
    No API limits, no rate limits, completely free and offline.
    """
    
    def __init__(self, model="llama3.2"):
        """
        Initialize Ollama summarizer.
        
        Args:
            model: Ollama model to use (default: llama3.2)
        """
        self.model = model
        
        if not check_ollama_available():
            raise RuntimeError(
                "Ollama is not available. Please install Ollama:\n"
                "  brew install ollama\n"
                "  ollama pull llama3.2"
            )
        
        print(f"âœ… Ollama initialized: {model}")
    
    def generate_summary(self, transcript, language="ja", max_length=300):
        """
        Generate a summary of the transcript.
        
        Args:
            transcript: Full text transcription
            language: "ja" for Japanese, "en" for English
            max_length: Target summary length in characters
        
        Returns:
            str: Generated summary
        """
        print(f"ğŸ“ Generating summary ({language}) with Ollama...")
        
        # Truncate transcript if too long (Ollama context limit)
        max_input = 6000
        if len(transcript) > max_input:
            transcript = transcript[:max_input] + "..."
        
        if language == "ja":
            prompt = f"""ä»¥ä¸‹ã®ãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã®æ–‡å­—èµ·ã“ã—ã‚’{max_length}æ–‡å­—ç¨‹åº¦ã§æ—¥æœ¬èªã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚

è¦ç´„ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³:
- ä¸»è¦ãªãƒˆãƒ”ãƒƒã‚¯ã¨è­°è«–ã®ãƒã‚¤ãƒ³ãƒˆã‚’å«ã‚ã‚‹
- è©±è€…ã®é‡è¦ãªä¸»å¼µã‚„çµè«–ã‚’å«ã‚ã‚‹
- èª­ã¿ã‚„ã™ã„è‡ªç„¶ãªæ—¥æœ¬èªã§æ›¸ã
- ç®‡æ¡æ›¸ãã§ã¯ãªãã€æ®µè½å½¢å¼ã§æ›¸ã
- è¦ç´„ã®ã¿ã‚’å‡ºåŠ›ã—ã€ä»–ã®èª¬æ˜ã¯ä¸è¦

æ–‡å­—èµ·ã“ã—:
{transcript}

è¦ç´„:"""
        else:
            prompt = f"""Please summarize the following podcast transcript in about {max_length} characters.

Summary guidelines:
- Include main topics and key discussion points
- Highlight important claims and conclusions
- Write in natural, readable English
- Use paragraph format, not bullet points
- Output only the summary, no other explanation

Transcript:
{transcript}

Summary:"""
        
        result = ollama_generate(prompt, self.model, timeout=180)
        
        if result:
            print(f"âœ… Summary generated ({len(result)} chars)")
            return result
        else:
            return "Summary generation failed"
    
    def generate_chapter_titles(self, timestamps_text, transcript, language="ja"):
        """
        Generate better chapter titles from timestamps.
        
        Args:
            timestamps_text: Existing timestamps with raw text
            transcript: Full transcription for context
            language: "ja" or "en"
        
        Returns:
            str: Improved timestamps with better titles
        """
        print(f"ğŸ“‘ Generating chapter titles ({language}) with Ollama...")
        
        # Truncate if needed
        if len(transcript) > 3000:
            transcript = transcript[:3000] + "..."
        
        if language == "ja":
            prompt = f"""ä»¥ä¸‹ã®ãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æ”¹å–„ã—ã¦ãã ã•ã„ã€‚
å„ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã«ã€ãã®æ™‚é–“å¸¯ã®å†…å®¹ã‚’è¡¨ã™ç°¡æ½”ãªã‚¿ã‚¤ãƒˆãƒ«ï¼ˆ15-30æ–‡å­—ï¼‰ã‚’ã¤ã‘ã¦ãã ã•ã„ã€‚

ç¾åœ¨ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—:
{timestamps_text}

å‡ºåŠ›å½¢å¼ï¼ˆã“ã‚Œã ã‘ã‚’å‡ºåŠ›ï¼‰:
MM:SS ã‚¿ã‚¤ãƒˆãƒ«
MM:SS ã‚¿ã‚¤ãƒˆãƒ«
...

ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„:"""
        else:
            prompt = f"""Please improve the following podcast timestamps.
Add concise titles (15-30 characters) that describe the content at each timestamp.

Current timestamps:
{timestamps_text}

Output format (only this):
MM:SS Title
MM:SS Title
...

Output only the timestamps:"""
        
        result = ollama_generate(prompt, self.model, timeout=120)
        
        if result:
            print(f"âœ… Chapter titles generated")
            return result
        else:
            return timestamps_text
    
    def translate_to_english(self, text, text_type="summary"):
        """
        Translate Japanese text to English.
        
        Args:
            text: Japanese text to translate
            text_type: "summary" or "transcript" for context
        
        Returns:
            str: English translation
        """
        print(f"ğŸŒ Translating {text_type} to English with Ollama...")
        
        # Truncate if too long
        max_input = 4000
        if len(text) > max_input:
            text = text[:max_input] + "..."
        
        prompt = f"""Translate the following Japanese text to natural English.
Preserve the meaning and context accurately.
Output only the translation, no other text.

Japanese text:
{text}

English translation:"""
        
        result = ollama_generate(prompt, self.model, timeout=180)
        
        if result:
            print(f"âœ… Translation complete ({len(result)} chars)")
            return result
        else:
            return "*Translation unavailable*"


def main():
    """Test the summarizer."""
    summarizer = OllamaSummarizer()
    
    test_text = """
    ä»Šå›ã¯ã€ãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã®è‡ªå‹•è¦ç´„ã‚·ã‚¹ãƒ†ãƒ ã«ã¤ã„ã¦è©±ã—ã¦ã„ã¾ã™ã€‚
    Whisperã‚’ä½¿ã£ãŸæ–‡å­—èµ·ã“ã—ã¨ã€ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚’ä½¿ã£ãŸè¦ç´„ç”Ÿæˆã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€
    åŠ¹ç‡çš„ã«ãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã®å†…å®¹ã‚’æ•´ç†ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
    ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã¯å®Œå…¨ã«ãƒ­ãƒ¼ã‚«ãƒ«ã§å‹•ä½œã—ã€å¤–éƒ¨ã‚µãƒ¼ãƒ“ã‚¹ã«ä¾å­˜ã—ã¾ã›ã‚“ã€‚
    """
    
    summary = summarizer.generate_summary(test_text, language="ja")
    print(f"\nGenerated Summary:\n{summary}")


if __name__ == "__main__":
    main()
